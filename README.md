# üöÄ Open Web UI + Ollama - Stack de IA Local

Este projeto fornece uma stack completa de IA local rodando no Docker, incluindo:

- **Open Web UI**: Interface web moderna para interagir com modelos de IA
- **Ollama**: Servidor local para executar modelos de linguagem (LLMs)

## üìã Pr√©-requisitos

- Linux Mint / Ubuntu (ou distribui√ß√£o compat√≠vel)
- 8GB+ de RAM recomendado
- 20GB+ de espa√ßo livre em disco
- Conex√£o com internet para download inicial

## üõ†Ô∏è Instala√ß√£o R√°pida

1. **Clone ou baixe todos os arquivos do projeto:**
   ```bash
   mkdir AutomaLLama
   cd AutomaLLama
   # Coloque todos os arquivos aqui
   ```

2. **Execute o script de configura√ß√£o:**
   ```bash
   chmod +x setup.sh
   ./setup.sh
   ```

3. **Inicie os servi√ßos:**
   ```bash
   docker-compose up -d
   ```

4. **Baixe alguns modelos do Ollama:**
   ```bash
   # Modelo pequeno e r√°pido (1.3GB)
   docker exec -it ollama ollama pull llama3.2:1b
   
   # Modelo m√©dio e balanceado (2GB)
   docker exec -it ollama ollama pull llama3.2:3b
   
   # Modelo portugu√™s otimizado (4GB)
   docker exec -it ollama ollama pull sabia-2:7b
   ```

## üåê URLs de Acesso

- **Open Web UI**: http://localhost:3000 
- **API Ollama**: http://localhost:11434

## üéØ Primeiro Uso

1. Acesse http://localhost:3000
2. Crie sua conta (primeira vez)
3. O Ollama ser√° detectado automaticamente
4. Comece a conversar com a IA!

## üìÅ Estrutura do Projeto

```
AutomaLLama/
‚îú‚îÄ‚îÄ docker-compose.yml      # Configura√ß√£o principal dos servi√ßos
‚îú‚îÄ‚îÄ setup.sh               # Script de instala√ß√£o autom√°tica
‚îú‚îÄ‚îÄ README.md              # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ .env                   # Vari√°veis de ambiente (gerado automaticamente)
```

## üîß Comandos √öteis

### Gerenciar Servi√ßos
```bash
# Iniciar todos os servi√ßos
docker-compose up -d

# Parar todos os servi√ßos
docker-compose down

# Reiniciar um servi√ßo espec√≠fico
docker-compose restart open-webui

# Ver status dos servi√ßos
docker-compose ps
```

### Logs e Troubleshooting
```bash
# Ver logs de todos os servi√ßos
docker-compose logs -f

# Ver logs de um servi√ßo espec√≠fico
docker-compose logs -f ollama

# Ver logs das √∫ltimas 50 linhas
docker-compose logs --tail=50
```

### Gerenciar Modelos Ollama
```bash
# Listar modelos instalados
docker exec -it ollama ollama list

# Baixar um modelo
docker exec -it ollama ollama pull nome-do-modelo

# Remover um modelo
docker exec -it ollama ollama rm nome-do-modelo

# Testar um modelo via CLI
docker exec -it ollama ollama run llama3.2:3b
```

## üß† Modelos Recomendados

### Para In√≠cio (baixo consumo de RAM):
- `llama3.2:1b` - 1.3GB, muito r√°pido
- `llama3.2:3b` - 2GB, balanceado

### Para Uso Geral (RAM moderada):
- `llama3.1:8b` - 4.7GB, muito bom
- `mistral:7b` - 4.1GB, eficiente

### Para Hardware Potente:
- `llama3.1:70b` - 40GB, excelente qualidade
- `codellama:13b` - 7.4GB, focado em c√≥digo

### Modelos em Portugu√™s:
- `sabia-2:7b` - Otimizado para portugu√™s
- `llama3.2:3b` - Funciona bem em portugu√™s

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Personalizar Open Web UI

Edite as vari√°veis de ambiente no `docker-compose.yml`:

```yaml
environment:
  - WEBUI_NAME=Minha IA Local
  - DEFAULT_LOCALE=pt-BR
  - ENABLE_SIGNUP=false  # Desabilitar cadastros
```

### Usar GPU (NVIDIA)

Para usar GPU com Ollama, modifique o docker-compose.yml:

```yaml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

## üîí Seguran√ßa

- Altere as chaves secretas em `.env` antes do uso em produ√ß√£o
- Configure firewall para limitar acesso √†s portas
- Use proxy reverso (nginx) para exposi√ß√£o externa
- Configure autentica√ß√£o forte no Open Web UI

## üêõ Solu√ß√£o de Problemas

### Ollama n√£o carrega modelos
```bash
# Verificar logs
docker-compose logs ollama

# Verificar espa√ßo em disco
df -h

# Reiniciar servi√ßo
docker-compose restart ollama
```

### Problemas de mem√≥ria
```bash
# Verificar uso de RAM
docker stats

# Usar modelos menores
docker exec -it ollama ollama pull llama3.2:1b
```

## üìä Monitoramento

### Verificar uso de recursos:
```bash
# Ver uso de CPU/RAM por container
docker stats

# Ver uso de disco
docker system df

# Limpar cache
docker system prune
```

## üîÑ Backup e Restaura√ß√£o

### Fazer backup dos dados:
```bash
# Backup dos volumes
docker run --rm -v ai-local-stack_ollama_data:/data -v $(pwd):/backup alpine tar czf /backup/ollama-backup.tar.gz -C /data .
docker run --rm -v ai-local-stack_open_webui_data:/data -v $(pwd):/backup alpine tar czf /backup/webui-backup.tar.gz -C /data .
```

### Restaurar backup:
```bash
# Restaurar volumes
docker run --rm -v ai-local-stack_ollama_data:/data -v $(pwd):/backup alpine tar xzf /backup/ollama-backup.tar.gz -C /data
docker run --rm -v ai-local-stack_open_webui_data:/data -v $(pwd):/backup alpine tar xzf /backup/webui-backup.tar.gz -C /data
```

## üÜò Suporte

Se encontrar problemas:

1. Verifique os logs: `docker-compose logs -f`
2. Consulte a documenta√ß√£o oficial:
   - [Open Web UI](https://github.com/open-webui/open-webui)
   - [Ollama](https://ollama.ai/)
3. Verifique issues no GitHub dos projetos

## üìú Licen√ßa

Este projeto √© fornecido como est√°, para uso educacional e pessoal. Respeite as licen√ßas dos componentes individuais.

---

**Aproveite sua IA local! üéâ**